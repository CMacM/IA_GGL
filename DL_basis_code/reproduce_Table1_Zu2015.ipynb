{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/halotools/sim_manager/halo_table_cache.py:12: UserWarning: Some of the functionality of the HaloTableCache classrequires h5py to be installed.\n",
      "  warn(\"Some of the functionality of the HaloTableCache class\"\n",
      "/usr/local/lib/python2.7/dist-packages/halotools/sim_manager/user_supplied_ptcl_catalog.py:13: UserWarning: Most of the functionality of the sim_manager sub-package requires h5py to be installed,\n",
      "which can be accomplished either with pip or conda\n",
      "  warn(\"Most of the functionality of the sim_manager \"\n",
      "/usr/local/lib/python2.7/dist-packages/halotools/sim_manager/download_manager.py:39: UserWarning: Some of the functionality of the DownloadManager requires h5py to be installed,\n",
      "which can be accomplished either with pip or conda\n",
      "  warn(\"Some of the functionality of the DownloadManager requires h5py to be installed,\\n\"\n",
      "/usr/local/lib/python2.7/dist-packages/halotools/sim_manager/cached_halo_catalog.py:15: UserWarning: Most of the functionality of the sim_manager sub-package requires h5py to be installed,\n",
      "which can be accomplished either with pip or conda. \n",
      "  warn(\"Most of the functionality of the \"\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shared_functions_wlp_wls as shared\n",
    "import scipy.integrate\n",
    "import pyccl as ccl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SURVEY = 'SDSS'\n",
    "if (SURVEY=='SDSS'):\n",
    "    import params as pa\n",
    "elif (SURVEY=='LSST_DESI'):\n",
    "    import params_LSST_DESI as pa\n",
    "else:\n",
    "    print \"We don't have support for that survey yet; exiting.\"\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: see how well we reproduce columns 5-8 of Table 1 from Zu & Mandelbaum 2015 ($f_{\\rm sat}$, $<b_{\\rm all}>$, $<b_{\\rm sat}>$, $<b_{\\rm cen}>$ for different stellar mass bins)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the cosmology\n",
    "OmM = pa.OmC_s + pa.OmB_s; OmB=pa.OmB_s; h0 = pa.HH0_s / 100.; sigma80=pa.sigma8_s; n_s0 = pa.n_s_s;\n",
    "\n",
    "p = ccl.Parameters(Omega_c = OmM-OmB, Omega_b = OmB, h = h0, sigma8=sigma80, n_s=n_s0)\n",
    "cosmo = ccl.Cosmology(p)\n",
    "\n",
    "rho_crit = 3. * 10**10 * pa.mperMpc / (8. * np.pi * pa.Gnewt * pa.Msun)  # Msol h^2 / Mpc^3, for use with M in Msol / h (comoving distances)\n",
    "rho_m = (OmM) * rho_crit # units of Msol h^2 / Mpc^3 (comoving distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define stellar mass bin limits and z's of bins:\n",
    "Mslow = np.asarray([8.5, 9.4, 9.8, 10.2, 10.6, 11.0, 11.2, 11.4, 12.])\n",
    "z = np.asarray([0.02, 0.04, 0.05, 0.08, 0.12, 0.15, 0.17, 0.19])\n",
    "\n",
    "# Define a halo mass range\n",
    "lgMh = np.linspace(10., 15., 30)\n",
    "Mh = 10**(lgMh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# At each redshift get the halo mass function and halo bias:\n",
    "HMF = [0]*len(z); bh = [0]*len(z)\n",
    "for zi in range(0,len(z)):\n",
    "    HMF[zi]= ccl.massfunction.massfunc(cosmo, Mh / h0, 1./ (1. + z[zi]), odelta=200.) / h0**3\n",
    "    bh[zi] = ccl.massfunction.halo_bias(cosmo, Mh / h0, 1./(1.+z[zi]), odelta=200.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get $<N_{\\rm sat}>$ and $<N_{\\rm cen}>$ with $M_*$ threshold of each stellar mass bin edge and subtract appropriately to get it in the stellar mass bins plotted in Figure 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Nsat_thresh = [0]*len(Mslow)\n",
    "Ncen_thresh = [0]*len(Mslow)\n",
    "for mi in range(0,len(Mslow)):\n",
    "    Nsat_thresh[mi] = shared.get_Nsat_Zu(Mh, 10**Mslow[mi], 'tot', 'SDSS')\n",
    "    Ncen_thresh[mi] = shared.get_Ncen_Zu(Mh, 10**Mslow[mi], 'SDSS')\n",
    "    \n",
    "Nsat_bin = [0] *(len(Mslow) - 1)\n",
    "Ncen_bin = [0] *(len(Mslow) - 1)\n",
    "for bi in range(0, len(Mslow)-1):\n",
    "    Nsat_bin[bi] = Nsat_thresh[bi] - Nsat_thresh[bi+1]\n",
    "    Ncen_bin[bi] = Ncen_thresh[bi] - Ncen_thresh[bi+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get $f_{\\rm sat}$ for each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get satelite fraction integrated over mass in each bin\n",
    "Nsat_intM = np.zeros(len(z))\n",
    "Ncen_intM = np.zeros(len(z))\n",
    "fsat = np.zeros(len(z))\n",
    "for zi in range(0,len(z)):\n",
    "    Nsat_intM[zi] = scipy.integrate.simps(Nsat_bin[zi] * HMF[zi], np.log10(Mh / h0))\n",
    "    Ncen_intM[zi] = scipy.integrate.simps(Ncen_bin[zi] * HMF[zi], np.log10(Mh / h0))\n",
    "    fsat[zi] = Nsat_intM[zi] / (Nsat_intM[zi] + Ncen_intM[zi])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each bin get the large scale bias for each population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First need to get the total number density for each population\n",
    "nbar = np.zeros(len(z))\n",
    "nbar_sat = np.zeros(len(z))\n",
    "nbar_cen = np.zeros(len(z))\n",
    "for zi in range(0,len(z)):\n",
    "    nbar[zi] = scipy.integrate.simps(HMF[zi] * (Ncen_bin[zi] + Nsat_bin[zi]), np.log10(Mh / h0))\n",
    "    nbar_sat[zi] = scipy.integrate.simps(HMF[zi] * Nsat_bin[zi], np.log10(Mh / h0))\n",
    "    nbar_cen[zi] = scipy.integrate.simps(HMF[zi] * Ncen_bin[zi], np.log10(Mh / h0))\n",
    "\n",
    "b_all = np.zeros(len(z))\n",
    "b_sat = np.zeros(len(z))\n",
    "b_cen = np.zeros(len(z))\n",
    "for zi in range(0,len(z)):\n",
    "    b_all[zi] = scipy.integrate.simps(HMF[zi] * bh[zi] * (Ncen_bin[zi] + Nsat_bin[zi]), np.log10(Mh/h0)) / nbar[zi]\n",
    "    b_sat[zi] = scipy.integrate.simps(HMF[zi] * bh[zi] * Nsat_bin[zi], np.log10(Mh/h0)) / nbar_sat[zi]\n",
    "    b_cen[zi] = scipy.integrate.simps(HMF[zi] * bh[zi] * Ncen_bin[zi], np.log10(Mh/h0)) / nbar_cen[zi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now output what we have calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.5          0.41295794   1.11693896   1.63878521   0.74984337]\n",
      " [  9.4          0.40620852   1.14988214   1.67057363   0.7936808 ]\n",
      " [  9.8          0.40701527   1.19681983   1.71019859   0.84444483]\n",
      " [ 10.2          0.36112081   1.26693256   1.82631883   0.95074443]\n",
      " [ 10.6          0.25495285   1.4249572    2.11390202   1.1892024 ]\n",
      " [ 11.           0.15264859   1.79879035   2.70559263   1.63543181]\n",
      " [ 11.2          0.09547071   2.22272771   3.29722221   2.10931761]\n",
      " [ 11.4          0.04459227   2.87725644   4.05754522   2.82216818]]\n"
     ]
    }
   ],
   "source": [
    "output = np.column_stack((Mslow[0:-1], fsat, b_all, b_sat, b_cen))\n",
    "print output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Okay so that's all basically correct, up to small factors, which I think are probably explainable by me guessing at redshifts for each bin. That's also not too surprising, given these quantities are largely related to large scales and large scales are not the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
