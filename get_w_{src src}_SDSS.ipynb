{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/halotools/sim_manager/halo_table_cache.py:12: UserWarning: Some of the functionality of the HaloTableCache classrequires h5py to be installed.\n",
      "  warn(\"Some of the functionality of the HaloTableCache class\"\n",
      "/usr/local/lib/python2.7/dist-packages/halotools/sim_manager/user_supplied_ptcl_catalog.py:13: UserWarning: Most of the functionality of the sim_manager sub-package requires h5py to be installed,\n",
      "which can be accomplished either with pip or conda\n",
      "  warn(\"Most of the functionality of the sim_manager \"\n",
      "/usr/local/lib/python2.7/dist-packages/halotools/sim_manager/download_manager.py:39: UserWarning: Some of the functionality of the DownloadManager requires h5py to be installed,\n",
      "which can be accomplished either with pip or conda\n",
      "  warn(\"Some of the functionality of the DownloadManager requires h5py to be installed,\\n\"\n",
      "/usr/local/lib/python2.7/dist-packages/halotools/sim_manager/cached_halo_catalog.py:15: UserWarning: Most of the functionality of the sim_manager sub-package requires h5py to be installed,\n",
      "which can be accomplished either with pip or conda. \n",
      "  warn(\"Most of the functionality of the \"\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shared_functions_wlp_wls as shared\n",
    "import scipy.integrate\n",
    "import pyccl as ccl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SURVEY = 'SDSS'\n",
    "if (SURVEY=='SDSS'):\n",
    "    import params as pa\n",
    "elif (SURVEY=='LSST_DESI'):\n",
    "    import params_LSST_DESI as pa\n",
    "else:\n",
    "    print \"We don't have support for that survey yet; exiting.\"\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: produce the projected correlation function of srcs alone for SDSS shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize the cosmology\n",
    "OmM = pa.OmC_s + pa.OmB_s; OmB=pa.OmB_s; h0 = pa.HH0_s / 100.; sigma80=pa.sigma8_s; n_s0 = pa.n_s_s;\n",
    "\n",
    "p = ccl.Parameters(Omega_c = OmM-OmB, Omega_b = OmB, h = h0, sigma8=sigma80, n_s=n_s0)\n",
    "cosmo = ccl.Cosmology(p)\n",
    "\n",
    "rho_crit = 3. * 10**10 * pa.mperMpc / (8. * np.pi * pa.Gnewt * pa.Msun)  # Msol h^2 / Mpc^3, for use with M in Msol / h (comoving distances)\n",
    "rho_m = (OmM) * rho_crit # units of Msol h^2 / Mpc^3 (comoving distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define stellar mass bin limits and z's of bins:\n",
    "Mslow = 6.8 * 10**9 # This what we get for SDSS srcs\n",
    "z = 0.28 # Set to the effective redshift for now.\n",
    "\n",
    "# Define a halo mass range\n",
    "lgMh = np.linspace(10., 15., 30)\n",
    "Mh = 10**(lgMh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HMF= ccl.massfunction.massfunc(cosmo, Mh / h0, 1./ (1. + z), odelta=200.) / h0**3\n",
    "bh= ccl.massfunction.halo_bias(cosmo, Mh / h0, 1./(1.+z), odelta=200.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get $<N_{\\rm sat}>$ and $<N_{\\rm cen}>$ with our $M_*$ threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Nsat = shared.get_Nsat_Zu(Mh, Mslow, 'tot', 'SDSS')\n",
    "Ncen = shared.get_Ncen_Zu(Mh, Mslow, 'SDSS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get the 1-halo term of the power spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbar= scipy.integrate.simps(HMF* (Ncen + Nsat), np.log10(Mh / h0))\n",
    "\n",
    "# Get the pair terms for each bin\n",
    "Ncensat = Ncen * Nsat\n",
    "# For the sat-sat case have to make sure we don't accidentally set negative\n",
    "Nsatsat= np.zeros(len(Mh))\n",
    "for mi in range(0,len(Mh)):\n",
    "    if (Nsat[mi]>1.0):\n",
    "        Nsatsat[mi] = Nsat[mi] * (Nsat[mi] -1.)\n",
    "    else:\n",
    "        Nsatsat[mi] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the Fourier transformed NFW\n",
    "logkmin = -6; kpts =40000; logkmax = 5; \n",
    "kvec_FT = np.logspace(logkmin, logkmax, kpts)\n",
    "# Actually we will use a downsampled version of this:\n",
    "k = np.logspace(np.log10(kvec_FT[0]), np.log10(kvec_FT[-1]), 40)\n",
    "y = shared.gety_ls(Mh, k, 'SDSS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 1-halo power spectrum in each bin\n",
    "Pkgg_1h= np.zeros(len(k))\n",
    "for ki in range(0,len(k)):\n",
    "    Pkgg_1h[ki] = scipy.integrate.simps(HMF*(Ncensat*y[ki, :] + Nsatsat*y[ki, :]**2), np.log10(Mh /h0)) / nbar**2 \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get the 2halo power spectrum in each bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the linear power spectrum from CCL in each bin\n",
    "Pklin = ccl.power.linear_matter_power(cosmo, k * h0, 1. / (1. + z)) * h0**3\n",
    "\n",
    "# Get the 2-halo power spectrum in each bin\n",
    "Pkgg_2h = np.zeros(len(k))\n",
    "for ki in range(0,len(k)):\n",
    "    Pkgg_2h[ki] = (scipy.integrate.simps(HMF * bh * y[ki, :] * (Ncen+ Nsat), np.log10(Mh /h0)) / nbar)**2 * Pklin[ki]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine 1 and 2 halo terms and output to file for each bin so we can Fourier transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Pgg_tot=np.zeros(len(k))\n",
    "for ki in range(0,len(k)):\n",
    "    if k[ki]>10**(-2):\n",
    "        Pgg_tot[ki] = Pkgg_2h[ki] + Pkgg_1h[ki]\n",
    "    else:\n",
    "        Pgg_tot[ki] = Pkgg_2h[ki]\n",
    "    \n",
    "# This needs to be interms of the more well-sampled k vector\n",
    "interp_in_k = scipy.interpolate.interp1d(np.log(k), np.log(Pgg_tot))\n",
    "full_Pgg = np.exp(interp_in_k(np.log(kvec_FT)))\n",
    "    \n",
    "save_P1h2h = np.column_stack((kvec_FT, full_Pgg))\n",
    "np.savetxt('./txtfiles/halofit_Pk/Pgg_srcsrc.txt', save_P1h2h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import $\\xi_{gg}(r)$ as Fourier transformed using FFTlog from our power spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r, xigg= np.loadtxt('./txtfiles/halofit_xi/xigg_srcsrc.txt', unpack=True)\n",
    "for ri in range(0,len(r)):\n",
    "    if xigg[ri]<10**(-12):\n",
    "        xigg[ri]=10**(-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project to get $w_{gg}$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interp_xi = scipy.interpolate.interp1d(np.log(r), np.log(xigg))\n",
    "    \n",
    "rp = np.logspace(-2, 2, 100)\n",
    "Pi = np.logspace(-5, np.log10(80.), 1000) # use the same projection length as Reid & Spergel 2008\n",
    "\n",
    "# For each bin get xi in terms of rp and Pi\n",
    "logxi_2d = np.zeros((len(rp), len(Pi)))\n",
    "for ri in range(0,len(rp)):\n",
    "    for pi in range(0,len(Pi)):\n",
    "        r_2d = np.sqrt( rp[ri]**2 + Pi[pi]**2 )\n",
    "        logxi_2d[ri, pi] = interp_xi(np.log(r_2d))\n",
    "\n",
    "# Now project at each rp and in each bin            \n",
    "wgg= np.zeros(len(rp))\n",
    "for ri in range(0,len(rp)):\n",
    "    wgg[ri] = 2.*scipy.integrate.simps(np.exp(logxi_2d[ri,:]), Pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now output what we have calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.loglog(rp, wgg, 'b+')\n",
    "plt.ylim(5,10000)\n",
    "plt.xlim(0.03,25)\n",
    "plt.ylabel('$w_p$')\n",
    "plt.xlabel('$r_p$')\n",
    "plt.title('$w_{gg}$')\n",
    "plt.savefig('./plots/w_srcsrc.pdf')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Okay so that's all basically correct, up to small factors, which I think are probably explainable by me guessing at redshifts for each bin. That's also not too surprising, given these quantities are largely related to large scales and large scales are not the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
