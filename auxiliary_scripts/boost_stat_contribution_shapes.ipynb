{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what what percentage of the fractional statistical error is made up of boost statistical error for different boost fractional errors (to see if boost statistical error is negligible compared to shape noise).\n",
    "\n",
    "For the shape-measurements methods method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.integrate \n",
    "import scipy.interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "import constrain_IA_shapes_FudgeFactor as sh\n",
    "\n",
    "SURVEY = 'LSST_DESI'\n",
    "\n",
    "if SURVEY=='SDSS':\n",
    "    import params as pa\n",
    "elif SURVEY=='LSST_DESI':\n",
    "    import params_LSST_DESI as pa\n",
    "else:\n",
    "    print \"We don't have support for that survey yet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boost fid= [ 6.92854945  3.98930454  2.50727285  1.76        1.38320865  1.1932222\n",
      "  1.09742687]\n"
     ]
    }
   ],
   "source": [
    "# Get the boost\n",
    "boost_fid = sh.get_boost(sh.rp_cents, pa.boost_assoc)\n",
    "print \"boost fid=\", boost_fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shear cov= [  1.45880562e-06   4.57400723e-07   1.31385285e-07   3.37901365e-08\n",
      "   7.76192077e-09   1.62438835e-09   3.18852547e-10]\n"
     ]
    }
   ],
   "source": [
    "# Get the shape noise for both samples\n",
    "shear_cov_1 = sh.shapenoise_cov(pa.e_rms_mean, sh.z_close_low, sh.z_close_high, boost_fid, sh.rp_cents, sh.rp_bins, pa.dNdzpar_fid, pa.pzpar_fid) \n",
    "shear_cov_2 = sh.shapenoise_cov(pa.e_rms_mean, sh.z_close_low, sh.z_close_high, boost_fid, sh.rp_cents, sh.rp_bins, pa.dNdzpar_fid, pa.pzpar_fid)\n",
    "\n",
    "print \"shear cov=\", shear_cov_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shear_covar = [0]*len(pa.cov_perc)\n",
    "for i in range(0,len(pa.cov_perc)):\n",
    "    shear_covar[i] = sh.get_cov_btw_methods(shear_cov_1, shear_cov_2, pa.cov_perc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Ncorr\n",
    "corr_fac_fid = sh.N_corr(sh.rp_cents, pa.dNdzpar_fid, pa.pzpar_fid, pa.dNdzpar_fid, pa.pzpar_fid, boost_fid) \n",
    "print \"corr_fac_fid=\", corr_fac_fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the boost statistical error as a fractional error on (B-1), since this is what I think we would measure. \n",
    "# Abs error on boost = abs error on boost - 1\n",
    "boost_err = 0.1 * (boost_fid-1.)\n",
    "\n",
    "# Compute the statistical error on Ncorr due to boost\n",
    "sumW_insamp = sh.sum_weights(sh.z_close_low, sh.z_close_high, pa.zmin, pa.zmax, sh.z_close_low, sh.z_close_high, pa.zmin_ph, pa.zmax_ph, pa.e_rms_mean, sh.rp_cents, pa.dNdzpar_fid, pa.pzpar_fid)\n",
    "sumW_intotal = sh.sum_weights(pa.zmin, pa.zmax, pa.zmin, pa.zmax, sh.z_close_low, sh.z_close_high, pa.zmin_ph, pa.zmax_ph, pa.e_rms_mean, sh.rp_cents, pa.dNdzpar_fid, pa.pzpar_fid)\n",
    "\n",
    "sig_Ncorr = (boost_err / boost_fid**2) * np.sqrt(1. - np.asarray(sumW_insamp)/ np.asarray(sumW_intotal))\n",
    "\n",
    "print \"sigNcorr=\", sig_Ncorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fiducial gamma IA\n",
    "gIA_fid= sh.gamma_fid(sh.rp_cents)\n",
    "print \"gIAfid=\", gIA_fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And get the percentage of the total statistical error that is boost statistical error, for a variety of choices of cov_perc and a\n",
    "\n",
    "cov_mat_stat_tot = np.diag(np.zeros(len(shear_cov_1)))\n",
    "cov_mat_stat_boost = np.diag(np.zeros(len(shear_cov_1)))\n",
    "for ai in range(0,len(pa.a_con)):\n",
    "    for ci in range(0,len(pa.cov_perc)):\n",
    "        for ri in range(0,len(shear_cov_1)):\n",
    "            cov_mat_stat_tot[ri,ri] = (1.-pa.a_con[ai])**2 * gIA_fid[ri]**2 * (( sig_Ncorr[ri]**2 / corr_fac_fid[ri]**2)  + (sh.subtract_var(shear_cov_1[ri], shear_cov_2[ri], shear_covar[ci][ri]) / (corr_fac_fid[ri]**2 * (1.-pa.a_con[ai])**2 * gIA_fid[ri]**2)))\n",
    "            cov_mat_stat_boost[ri,ri] = (1.-pa.a_con[ai])**2 * gIA_fid[ri]**2 * ( sig_Ncorr[ri]**2 / corr_fac_fid[ri]**2)\n",
    "        frac = np.diag(cov_mat_stat_boost) / np.diag(cov_mat_stat_tot)\n",
    "        print \"a=\",pa.a_con[ai], \"%=\", pa.cov_perc[ci], \"percentage=\", frac*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
